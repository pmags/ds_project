[["index.html", "R Notebook Introduction to Data Science [Masted Data Science] Introduction", " R Notebook Introduction to Data Science [Masted Data Science] Pedro Miguel de Sousa Magalhães 2022-01-16 Introduction Students commitment Declaro que o presente relatório é de minha autoria e não foi utilizado previamente noutro curso ou unidade curricular, desta ou de outra instituição. As referências a outros autores (afirmações,ideias, pensamentos) respeitam escrupulosamente as regras da atribuição, e encontramse devidamente indicadas no texto e nas referências bibliográficas, de acordo com as normas dereferenciação. Tenho consciência de que a prática de plágio e auto-plágio constitui um ilícito académico. R session information The R session information when building this project is has shown below: sessionInfo() ## R version 3.5.3 (2019-03-11) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19042) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Portuguese_Portugal.1252 LC_CTYPE=Portuguese_Portugal.1252 ## [3] LC_MONETARY=Portuguese_Portugal.1252 LC_NUMERIC=C ## [5] LC_TIME=Portuguese_Portugal.1252 ## ## attached base packages: ## [1] stats graphics grDevices datasets utils methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.24 digest_0.6.29 R6_2.5.1 jsonlite_1.7.2 ## [5] magrittr_2.0.1 evaluate_0.14 stringi_1.7.6 rlang_0.4.12 ## [9] renv_0.15.0 rstudioapi_0.13 jquerylib_0.1.4 bslib_0.3.1 ## [13] rmarkdown_2.11 tools_3.5.3 stringr_1.4.0 xfun_0.29 ## [17] yaml_2.2.1 fastmap_1.1.0 compiler_3.5.3 htmltools_0.5.2 ## [21] knitr_1.37 sass_0.4.0 "],["business-understanding.html", "Stage 1 Business Understanding 1.1 Business objectives 1.2 Assess Situation 1.3 Data Mining Goals 1.4 Project Plan 1.5 Terms", " Stage 1 Business Understanding 1.1 Business objectives What is the company wishing to achieve? A Luxury high end Brand sells its goods online using its own e-commerce platform and wishes to increase its conversion rate. No further information is available about the companies business like other sale channel or product category, although from the available information we can conclude the brand has a global reach. The available sample data shows a 8% top of funnel conversion rate ( substantially higher than e-commerce benchmark which is around 2% ). Conversion top of funnel uses landing touchpoints as index. No information is given about any goal or additional metrics tracked by the company, therefore it wont be take into consideration during this project. 1.2 Assess Situation What is available for the project? A sample of balanced and unbalanced information was made available containing information about individual sessions. No information is provided about session definitions (eg: cap session duration. Industries norm is between 25 ~ 30 min) and about user anonymous ids. There isnt therefore, enough information to analyse potential impacts of different customer journeys. Additionally no information is provided concerning the date each session accured, which invalidate any seasonal analysis. Given the generaly accepted impact that multitudinous has on conversion it will be assume henceforward that sessions are independent and identically distributed 1.2.1 Hardware and environment special needs There arent any specific needs for hardware or data infrastructure. Data will be ingested directly from csv files and all transformation will be done on development machine. Environment information available on .renv file 1.3 Data Mining Goals How will Data team achieve Business Goals? Based on the available information the project is binary classification problem with a focus on inference.. The company is looking to identify session variables which signal users which will convert or not in order to redefine marketing strategies and approaches. There are only to available states, either the user bought or it didnt. No information is provide regarding the expected use of this project. Therefore it will be assumed through this project that the main stakeholders is the marketing team and are human. Therefore, a bigger focus on the modeling process and coefficients is needed and is considered part of the deliver. The assumption is relevant since it impacts the final conclusions and the final model chosen. Given 2 similar models with sensible the same target metric the more interpretable one will be chosen. Target metrics Given the goals of this project stated before on Business Objectives the focus will be on maximizing Accuracy. The business is looking for the model which more accurately explains session conversion. Data questions During this project the following questions will be addressed Is there a relationship between a conversion and information available related to that session ? Which is the contribution of each of the variables to conversion? How accurately can we estimate the effect on conversion? Is there synergy among each session elements? Does a model surpass a naive baseline approach of assuming the most shown class? Does Data imbalance impact output? 1.4 Project Plan Due to time restrictions the present project follows a hybrid approach of CRISP-DM and TDSF and is organized in the following stages: Stage 1 - Understand Business and Its environment Stage 2 - Understand and explore data: study of distribution and relationship among available data Stage 3 - Data preparation for modeling: Preprocessing workflow Stage 4 - Modeling: Model definition, initial experiments and hyperparameter tunning Stage 5 - Delivery: Conclusions of the project 1.5 Terms The following terms will be used during this project with the following meaning: User: any unique IP which has reached the store. One individual can have more than one ip, Client: a user that converted, this means, it bought from the shop, Touchpoints: represents any interaction between the user and the online store of any sort, Session: a period of time (normally of 30 min max) during which the user interacted with the shop. Every session starts with a touchpoint. Under some conditions depending on the website metrics collection a session can have more than one touchpoint. "],["data-aquisition-and-understanding.html", "Stage 2 Data aquisition and understanding 2.1 Description 2.2 Irrelevant or technical information 2.3 Target Variable: To buy or not to buy  2.4 Customer Behavior Features 2.5 Customer Journey related Features 2.6 Client Segment related Features 2.7 Device and Geography related Features 2.8 Summary of findings", " Stage 2 Data aquisition and understanding 2.1 Description Two datasets were made available for analysis. One with a balanced class for the target variable and the other raw. This analysis focus on the full dataset. From the available description the available columns follow into one of the following categories: Irrelevant for the project; Target or dependent Variable: what will be studied; Customer Behavior related Features: potential features which reflect how users behaved on each session and can explain the target variable; Customer Journey related Features: potential features contain information on how uses interacted with the on-line shop; Client Segment related Features: potential features which provide information on how a user might be classified from a marketing perspective; Device and Geography related Features: provide information regarding location and access device used by a user. str(unbalanced_data) ## tibble [95,000 x 20] (S3: tbl_df/tbl/data.frame) ## $ session_id : Factor w/ 95000 levels &quot;00012961-8ad6-412d-9a5b-9eada1b93c97&quot;,..: 63567 36472 84761 59471 82382 75884 38721 75862 6836 90844 ... ## $ plaform : Factor w/ 2 levels &quot;mobile_app&quot;,&quot;website&quot;: 2 2 1 1 2 2 2 2 1 2 ... ## $ segment : Factor w/ 6 levels &quot;FFACCESS-Bronze&quot;,..: 6 6 1 6 6 6 6 6 2 3 ... ## $ customer_type : Factor w/ 2 levels &quot;customer&quot;,&quot;prospect&quot;: 2 2 1 1 2 2 2 2 1 1 ... ## $ device_group : Factor w/ 3 levels &quot;App&quot;,&quot;Desktop&quot;,..: 3 3 1 1 3 3 3 3 1 2 ... ## $ visitor_type : Factor w/ 2 levels &quot;new&quot;,&quot;returning&quot;: 1 2 2 2 1 2 1 1 2 1 ... ## $ has_listing : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 2 2 1 1 1 2 2 2 ... ## $ has_used_search : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 2 2 ... ## $ has_recommendation : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 1 1 1 1 1 1 2 ... ## $ has_add_to_wishlist : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 2 1 1 1 1 1 1 ... ## $ has_add_to_bag : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ duration : num [1:95000] 0 0 27 88 0 128 0 0 9 342 ... ## $ view_qty : num [1:95000] 1 1 10 15 1 2 1 1 4 7 ... ## $ unique_product_qty : num [1:95000] 0 1 1 5 1 2 1 0 0 2 ... ## $ unique_browse_designer_qty: num [1:95000] 0 1 1 1 1 1 1 1 1 3 ... ## $ unique_browse_category_qty: Factor w/ 32 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 2 2 2 2 2 2 2 2 1 3 ... ## $ is_subscribed : Factor w/ 3 levels &quot;No&quot;,&quot;Unknown&quot;,..: NA NA 1 3 NA NA NA NA 3 3 ... ## $ browser_name : Factor w/ 51 levels &quot;AliApp&quot;,&quot;Android&quot;,..: 37 5 NA NA 21 37 28 5 NA 37 ... ## $ country : Factor w/ 162 levels &quot;AD&quot;,&quot;AE&quot;,&quot;AF&quot;,..: 154 111 134 134 154 134 11 73 128 154 ... ## $ bought : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... 2.2 Irrelevant or technical information Session_id is a unique key which sole purpose is to uniquely identify each sessions and contains no exploratory power, therefore it will be removed from future analysis. # test if a single session can have more than one row length( unique(unbalanced_data$session_id) ) == nrow(unbalanced_data) ## [1] TRUE 2.3 Target Variable: To buy or not to buy  Boolean variable which assumes the values of 1 when a transaction happened on the session and zero other wise. Its the objective variable of this project. The available dataset with full data presents a severe class imbalance towards not buy which can and will Bias model output (a model assuming that nobody buys on each session is already 92% of the times right). This bias is corrected on the balanced dataset at the expanse of some changes on grid.draw(balanced_grid) 2.4 Customer Behavior Features In this section we will explore the variables available using both unbalanced and balanced data. In special we will focus on the following questions: What type of variation occurs within my variables? What type of covariation occurs between my variables? 2.4.1 Duration Continuous variable representing the time a user was on a particular sessions. Values measured in seconds. No specifics are given regarding the concept of session used. The market standard is to limit session duration to an engagement window between 25 to 30 minutes. Above that time cap, a new session would start (even if the user is still navigating the site). It will assumed that no such cap exist and therefore extreme values are not due to technical issues on web metrics. Distribution information shows a severely left skewed distribution which is confirmed by the behavior of the qq_plot. This is present on both raw and balanced data. The max duration of 2.0295^{4} means that at least one session was {r} max(unbalanced_data$duration)/(60 * 60) hours long, which, given what was said regarding session cap wont be considered as an outlier due to error. Data shows that visit duration is a long tail with a great majority of users staying just a fraction but a long line of users (power users?) which stay for longer. This distribution does not allow for a good analysis of outliers of the distribution given its nature and might prove to be tricky during modeling due to its non Gaussian nature. density &lt;- ggplot(unbalanced_data, aes(x = duration)) + geom_density(color=&quot;darkblue&quot;) + theme_masterDS() + labs( x = &quot;&quot;, y = &quot;&quot;, title = &quot;Density&quot; ) boxplot &lt;- ggplot(unbalanced_data, aes(x = 1, y = duration)) + geom_boxplot() + theme_masterDS()+ labs( x = &quot;&quot;, y=&quot;&quot;, title = &quot;Boxplot&quot; ) density_log &lt;- ggplot(unbalanced_data, aes(x = log(duration))) + geom_density(color=&quot;darkblue&quot;) + theme_masterDS() + labs( x = &quot;&quot;, y = &quot;&quot;, title = &quot;Density (log transform)&quot; ) boxplot_log &lt;- ggplot(unbalanced_data, aes(x = 1, y = log(duration))) + geom_boxplot() + theme_masterDS()+ labs( x = &quot;&quot;, y=&quot;&quot;, title = &quot;Boxplot (log transform)&quot; ) grid.arrange(density, boxplot, density_log, boxplot_log, layout_matrix = rbind(c(1,2),c(3,4))) ## Warning: Removed 41417 rows containing non-finite values (stat_density). ## Warning: Removed 41417 rows containing non-finite values (stat_boxplot). As a preprocess step this feature will be log transformed. No more complex Box-Cox transformation will be used given that good results can already be achieved with the current transformation. No missing values were found. ## Warning: Removed 41417 rows containing non-finite values (stat_boxplot). ## Warning: Removed 3823 rows containing non-finite values (stat_boxplot). Ploted against the target variable it seems to imply a positive relationship between duration and conversion. From the sample sessions that converted had on median a higher duration. 2.4.2 Page View Quantity Discrete variable measuring the number of views during a session. Distribution information shows a severely left skewed distribution which is confirmed by the behavior of the qq_plot. This is present on both raw and balanced data. The max duration is of 1151. Despite is distribution the data shows a median of 10 page views which suggest an interesting engagement for a e-commerce provided other indicators regarding navigation are solid (if a user is jumping between pages because they dont load properly, an increase on page views might no be beneficial). Given the available information we cannot conclude that outliers due to errors exist. In an attempt to normalize the distribution a Box-Cox transformation will be used with a Optimized Lambda. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. With this transformation it is now visible 2 segments of sessions which deserve attention, sessions with zero views or just 1 view. The information available provides no information whatsoever regardingt definition of view. The standard (defined by google) is the first hit at log to be a pageview. Therefore, technically all sessions would have a view. Using the same approach, a 1 view session is technically called a bounce. Once again assuming Googles standard (it owns 97% of the analytics market ) a bounce is a session with just a starting hit but not information regarding log out (duration is a difference based calculation). On a standard scenario the approach would be to remove all observations with a zero pageviews (technical errors) and session with 1 page view (if not other information regarding duration is available). In the scenario in analysis, given the available information we cant assume zero page views to be a technical error and therefore will filter the dataset for observations with zero or one page views and a session duration smaller than 2 seconds. The balanced data to be used with modeling is already close to normal distribution. Compared by conversion the data seems to suggest that sessions with conversion had a higher number of page views. 2.4.3 Product Pages unique page views Discrete variable measuring the visits product related pages had. grid.draw(eda_unbalanced$unique_product_qty) grid.draw(eda_balanced$unique_product_qty) The initial graphic analysis shows a right skewed distribution similar to was seen on before on Pageviews. Nonetheless, in this scenario zero page views is a acceptable outcome. When the target class is taken into consideration it seems to sugest a teneous impact albeit its true effects might be hidden by the distribution skew. Ploted together target, pageviews and product pageviews it becomes more evident that the relationship between target and product pageviews is not very visable but the Chi test lets us renouce the null hypothesis. ## ## Pearson&#39;s Chi-squared test ## ## data: x and as.factor(unbalanced_data$bought) ## X-squared = 8099.1, df = 165, p-value &lt; 2.2e-16 On the other hand, there seems to have a strong relation positive relation between pageviews and pagevies product. This may suggest an interaction between both features. Given the nature of this feature we will explore the impact of converting into a ratio using page views as denominator since product page view is in fact a subgroup of pageviews. This transformation provides more insights into the behaviour of the average user indicating that on average 25% of page navigation is done on product related pages (from and into). Simultaneously it smooths the skewed effect reported earlier ## Warning: Removed 1833 rows containing non-finite values (stat_boxplot). If the e-commecer has a more transactional focus it might not come as surprice the above plot and it seems to imply that user might already know what they want to buy prior to visit. Similar transformation will be done to the subsquent variables- 2.4.4 Designer Browsing unique page views Discrete variable measuring the number of Designers researched. grid.draw(eda_unbalanced$unique_browse_designer_qty) grid.draw(eda_balanced$unique_browse_designer_qty) Similar approach done before This transformation provides more insights into the behaviour of the average user indicating that on average 25% of page navigation is done on product related pages (from and into). Simultaneously it smooths the skewed effect reported earlier ## Warning: Removed 1421 rows containing non-finite values (stat_boxplot). 2.4.5 Products categories unique id Discrete variable measuring the visits Designer related pages had. grid.draw(eda_unbalanced$unique_browse_category_qty) grid.draw(eda_balanced$unique_browse_category_qty) Similar approach done before ## NULL This transformation provides more insights into the behaviour of the average user indicating that on average 25% of page navigation is done on product related pages (from and into). Simultaneously it smooths the skewed effect reported earlier ## Warning: Removed 1542 rows containing non-finite values (stat_boxplot). 2.4.6 Potential ideas for feature engineering which were not implemented The available data could synthesized using a non-supervised cluster algorithm which would help identify subsets of behaviors to be used during modeling. This approach has the advantage of possible reduce 4 variables into one. 2.5 Customer Journey related Features In this section we will explore the variables available using both unbalanced and balanced data. In special we will focus on the following questions: What type of variation occurs within my variables? What type of covariation occurs between my variables? 2.5.1 Device Group Categorical variable with 3 levels each representing the device source for each session. App, Desktop, Mobile Web ## $device_group ## ## Pearson&#39;s Chi-squared test ## ## data: x and unbalanced_data$bought ## X-squared = 3755.4, df = 2, p-value &lt; 2.2e-16 ## $device_group ## ## Pearson&#39;s Chi-squared test ## ## data: x and balanced_data$bought ## X-squared = 2003.4, df = 2, p-value &lt; 2.2e-16 The majority of sessions had origin on a Mobile Web Platform, nonetheless, depite the traffic, graphical analysis sugests that Mobile Appliction has in factr higher convertion rates. This same conclusions can be extracted from the balanced dataset although the differences are not as evident. The \\(\\chi ^2\\) tests for both dataset leads into refusing the null hypothesis so it suggests that a degree of relation ship exists between both variables. This dataset includes a variable named platform that suggest that most sessions were done using website. At first glance it seems counter intuitive that most access be done through website and Mobile web but the above plot show that in reality most users opt to access using the mobile version of the website instead of the app. The \\(\\chi ^2\\) test between this 2 variables confirms that both variable seem to have a degree of relation leading to not refuting the null hypothesis of dependency. This implies interaction (or synergy on marketing terms) between features which can impact how modeling efforts specially for models dependent on linear transformations (least squares regression) as is the case of logit. During the modeling step one might consider removing one of the variables or generate a new compound feature. The chi-test confirms the strong relationship between the two since refute the null hypothesis. ## ## Pearson&#39;s Chi-squared test ## ## data: as.character(unbalanced_data$plaform) and as.character(unbalanced_data$device_group) ## X-squared = 95000, df = 2, p-value &lt; 2.2e-16 2.5.2 Platform Categorical variable with mobile_app, website representing the platform of origin of the session. prop.table(table(unbalanced_data[[&quot;plaform&quot;]])) ## ## mobile_app website ## 0.2761263 0.7238737 ## $plaform ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x and unbalanced_data$bought ## X-squared = 2896.4, df = 1, p-value &lt; 2.2e-16 prop.table(table(balanced_data[[&quot;plaform&quot;]])) ## ## mobile_app website ## 0.39125 0.60875 balanced_chi[&quot;plaform&quot;] ## $plaform ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x and balanced_data$bought ## X-squared = 1360.5, df = 1, p-value &lt; 2.2e-16 The above graphical analysis suggests that the majority of sessions (around 72% on the unbalanced dataset and 61% on the balanced ) were accessed through the website. Despite the inbalance between platforms the difference on conversion rate is visible between platforms. The \\(\\chi ^2\\) for both unbalanced and balanced datasets do not allow for the rejection of the null hypothesis implying the existence of a degree of linear regression between the 2 variables. 2.5.3 Journey milestones (has_listings) &gt; has_add_to_bag grid.draw(eda_unbalanced$has_add_to_wishlist) grid.draw(eda_unbalanced$has_listing) grid.draw(eda_unbalanced$has_recommendation) grid.draw(eda_unbalanced$has_used_search) grid.draw(eda_unbalanced$has_add_to_bag) These features are plotted together because they give insights into the journey a user made inside the website. We can conclude over a conversion funnel but it makes sense to explore the interactions between then while modeling because its known that normally strong effects exist between then (a user had a recommendation and added to the bag might be together a strong signal for conversion). The graphical analysis already provides a important insight given the business objectives. From the unbalanced data available we can conclude that around of 46% shopping carts are lost on that session. That raises a question of how are this recovered (example on a next session) or if this means that all this sales are lost right at the end of the sales funnel. prop.table(table(unbalanced_data[unbalanced_data$has_add_to_bag == 1,]$bought, unbalanced_data[unbalanced_data$has_add_to_bag == 1,]$has_add_to_bag)) [,2] ## 0 1 ## 0.4500879 0.5499121 2.6 Client Segment related Features 2.6.1 Customer type Categorical variable with customer, prospect. The graphical analysis shows inbalance towards prospect clients. When plotted together with target class it suggests a potential relationship with customers have a higher propency to buy again (happy customers?) The results from the \\(\\chi^2\\) hypothesis test does not refuse the null hypothesis reinforcing the graphical analysis that a relationship might exist between this 2 variables that implies that recurrent customers buy more. unbalanced_chi[&quot;customer_type&quot;] ## $customer_type ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x and unbalanced_data$bought ## X-squared = 9750.7, df = 1, p-value &lt; 2.2e-16 balanced_chi[&quot;customer_type&quot;] ## $customer_type ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x and balanced_data$bought ## X-squared = 3590, df = 1, p-value &lt; 2.2e-16 2.6.2 Segment Categorical variable with FFACCESS-Bronze, FFACCESS-Gold, FFACCESS-Platinum, FFACCESS-Private-Client, FFACCESS-Silver, without_segment levels. No information is provided regarding how this segment is generated and is highly unbalanced towards no segment. unbalanced_chi[&quot;segment&quot;] ## $segment ## ## Pearson&#39;s Chi-squared test ## ## data: x and unbalanced_data$bought ## X-squared = 10203, df = 5, p-value &lt; 2.2e-16 balanced_chi[&quot;segment&quot;] ## $segment ## ## Pearson&#39;s Chi-squared test ## ## data: x and balanced_data$bought ## X-squared = 3071, df = 5, p-value &lt; 2.2e-16 Despite the low number of sessions made by users with a segment the ones which have seem to have a very high conversion rate despite their level. The number of levels on a categorical variable affects the model, and in this case the extra information doesnt seem to be bringing any new impute therefore, it will be collapses into a new variable with just two levels, has segment or it has no segment. 2.6.3 Visitor type Categorical variable with 2 levels representing if a given user is a new or recurring user. It differs from customer type because it focus on visits and not actual conversion, therefore a returning user can be a prospect. ## $visitor_type ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x and unbalanced_data$bought ## X-squared = 4001.3, df = 1, p-value &lt; 2.2e-16 ## $visitor_type ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: x and balanced_data$bought ## X-squared = 2529.6, df = 1, p-value &lt; 2.2e-16 This feature focus on the Visitors. The unbalanced data available shows almost a 50% split, situations that changes on the balanced dataset which has a inbalance towards returning visitors. The graphical analysis suggests a higher conversion rate for returning visitor that for new one, suggesting that continuous visits (engagment) plays a role in conversion. The current dataset has information regarding customer type crossed with visitor type can provide us with interesting information ggplot(unbalanced_data, aes(x = visitor_type, fill = customer_type)) + geom_bar(stat = &quot;count&quot;) + theme_masterDS() + labs( x = &quot;&quot;, y = &quot;&quot;, title = &quot;Visitor Type x Custor Type&quot; ) Has expected only a fraction of new visitors actually buy on the first session hinting that the conversion journey is longer than one session, meaning than several visits are needed before a first conversion. We dont have enough information to conclude about the number of sessions needed (journey lenght) and neither the session index given a time window (eg: the current session is the #3 in the last 28 days) which is know to have a impact on conversion. 2.6.4 Is subscribed grid.draw(eda_unbalanced$is_subscribed) grid.draw(eda_balanced$is_subscribed) For both datasets the class majority is NA. From the information given we cannot conclude that we can remove this feature from the model, and given the number of observations affected we will look for imputation alternatives. If a particular variable is having more missing values that rest of the variables in the dataset, and, if by removing that one variable you can save many observations then is advisable to remove. But in this case, the fact a user is subscribed is a strong signal and one of the few (apart from segment) which provides insight about the companies marketing strategy. Removing this feature would mean removing signal regarding an ongoing engagement strategy. A concervative approach would be to consider all missing values as non subscribers. Given the context of this project an alternative root of statistical imputation will be used relying on a knn algorithm with a small number of neighbors (3 = 2 levels + 1) 2.7 Device and Geography related Features 2.7.1 Browser name Categorical variable representing the browser used as source for each session. From the available information from the unbalanced data we have 52 different browsers present on this dataset. Most of this levels have only a small number of sessions and will be colapsed during the preprocess ## [1] Safari Chrome &lt;NA&gt; ## [4] Instagram App Mobile Safari UIWebView Android WebView ## [7] Facebook App Firefox Yandex Browser ## [10] Miui Browser Google App Opera ## [13] Edge Samsung Browser HuaweiBrowser ## [16] Vivo Browser Line App Naver ## [19] WeChat App Opera Mobile DuckDuckGo Browser ## [22] Sogou Explorer AliApp Silk ## [25] Apple Mail Firefox for iOS WKBrowser ## [28] UC Browser HeyTapBrowser Maxthon ## [31] Whale Browser Snapchat CM Browser ## [34] Weibo Default Browser Tungsten Browser ## [37] QQBrowser Ecosia Sleipnir ## [40] Android RDDocuments App Coc Coc Browser ## [43] Meizu Browser DareBoost Bot Playstation Browser ## [46] Puffin Netease Music Waterfox ## [49] Elements Browser Iron Mail Master ## [52] Edge Mobile ## 51 Levels: AliApp Android Android WebView Apple Mail Chrome ... Yandex Browser 2.7.2 Country Categorical variable containing the country of origin for each session.Most of this levels have only a small number of sessions and will be colapsed during the preprocess ## [1] US MX RU AU IN PT KR CL GB AE CN HK DE GR PL ## [16] KW LI BR AT HR CA IT SA TW ZA VN JP FR RO QA ## [31] ME BH ID PE ES BE PK LB IE KZ BG PH NL AR IL ## [46] SE NZ GE DO DK BY AF MD TH MY UA LT CH EG NO ## [61] CO SG BA RS KH HU MO IS OM AM UY MA NG AL BB ## [76] EE DZ MK SK LU AO TR PA EC JO IQ BD GH CY SI ## [91] AZ CZ MU KG MT BN FI TN LV CR BS SV UZ GT AD ## [106] NP HN CM KE ET AI MC GP JM &lt;NA&gt; MW SN VE VI CI ## [121] MR ZM AW CG LK PR GU JE GG BM SR GI TT GY BW ## [136] AQ MN MQ HT LC SM GF KY BO SL LA IC KV VC TC ## [151] BJ MZ LS RE NC DM MV UG TG GL PY NI GM ## 162 Levels: AD AE AF AI AL AM AO AQ AR AT AU AW AZ BA BB BD BE BG BH BJ ... ZM 2.7.3 Dealing with Missing Data Somo missing data was identified and will be imputed using statistical imputation (refer to customer type analysis) md.pattern(unbalanced_data, rotate.names = TRUE) ## session_id plaform segment customer_type device_group visitor_type ## 7853 1 1 1 1 1 1 ## 60913 1 1 1 1 1 1 ## 19833 1 1 1 1 1 1 ## 6397 1 1 1 1 1 1 ## 2 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 ## 1 1 1 1 1 1 1 ## 0 0 0 0 0 0 ## has_listing has_used_search has_recommendation has_add_to_wishlist ## 7853 1 1 1 1 ## 60913 1 1 1 1 ## 19833 1 1 1 1 ## 6397 1 1 1 1 ## 2 1 1 1 1 ## 1 1 1 1 1 ## 1 1 1 1 1 ## 0 0 0 0 ## has_add_to_bag duration view_qty unique_product_qty ## 7853 1 1 1 1 ## 60913 1 1 1 1 ## 19833 1 1 1 1 ## 6397 1 1 1 1 ## 2 1 1 1 1 ## 1 1 1 1 1 ## 1 1 1 1 1 ## 0 0 0 0 ## unique_browse_designer_qty unique_browse_category_qty bought country ## 7853 1 1 1 1 ## 60913 1 1 1 1 ## 19833 1 1 1 1 ## 6397 1 1 1 1 ## 2 1 1 1 0 ## 1 1 1 1 0 ## 1 1 1 1 0 ## 0 0 0 4 ## browser_name is_subscribed ## 7853 1 1 0 ## 60913 1 0 1 ## 19833 0 1 1 ## 6397 0 0 2 ## 2 1 0 2 ## 1 0 1 2 ## 1 0 0 3 ## 26232 67313 93549 md.pattern(balanced_data,rotate.names = TRUE) ## session_id plaform segment customer_type device_group visitor_type ## 3676 1 1 1 1 1 1 ## 6064 1 1 1 1 1 1 ## 5649 1 1 1 1 1 1 ## 611 1 1 1 1 1 1 ## 0 0 0 0 0 0 ## has_listing has_used_search has_recommendation has_add_to_wishlist ## 3676 1 1 1 1 ## 6064 1 1 1 1 ## 5649 1 1 1 1 ## 611 1 1 1 1 ## 0 0 0 0 ## has_add_to_bag duration view_qty unique_product_qty ## 3676 1 1 1 1 ## 6064 1 1 1 1 ## 5649 1 1 1 1 ## 611 1 1 1 1 ## 0 0 0 0 ## unique_browse_designer_qty unique_browse_category_qty country bought ## 3676 1 1 1 1 ## 6064 1 1 1 1 ## 5649 1 1 1 1 ## 611 1 1 1 1 ## 0 0 0 0 ## browser_name is_subscribed ## 3676 1 1 0 ## 6064 1 0 1 ## 5649 0 1 1 ## 611 0 0 2 ## 6260 6675 12935 2.8 Summary of findings Features Device Group and Platform showed a strong relationship between them. Given that Platform provides more information Device Group will be removed Features starting with has_ refere to milestones on customer journey, therefore the need to include all of them should be taken in consideration. We will use PCA to merge this into one feature and experiment between the use of them individual or together, Duration feature is Right Skewed and contains observations which lack business sense. To avoid any impact on the model, specially on ones more sensible to variance, it will be log transformed and observations above 24hours will be removed Replace all Missing values on country, browser_name and is_subscribed using Nearest Neighbors imputation with a very a k of 3, Reduce the number of observations of class country and browser_name by compressing smaller ones into a generic class named others. This will be done because some models do not manage very well the fact that some dummy variables only have one level, Reduce the number of levels of Segment by re factoring into a class of has or not segment, Specially for the use of Neural Networks and Supervisor Vector Machines, numeric variables will be normalized and scaled Converts unique_browse_designer_qty, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty to ratios over Total Pageviews (view_qty) which provides information on the amount of page views actually related to that category and subsquently log transform, Sessions with duration lower than 2 seconds and a total page view of 0 or 1 will be removed, The graphical analysis already provides a important insight given the business objectives. From the unbalanced data available we can conclude that around of 46% shopping carts are lost on that session. That raises a question of how are this recovered (example on a next session) or if this means that all this sales are lost right at the end of the sales funnel. "],["data-transformations.html", "Stage 3 Data transformations 3.1 Summary of transformation steps identified during EDA", " Stage 3 Data transformations # improt libraries library(tidymodels) library(tidyverse) # import and prepare data # import data unbalanced_data &lt;- read_csv(&quot;./data/train_full.csv&quot;) %&gt;% select(-...1, -session_id) %&gt;% mutate(bought = as.factor(bought)) ## New names: ## * `` -&gt; ...1 ## Rows: 95000 Columns: 21 ## -- Column specification -------------------------------------------------------- ## Delimiter: &quot;,&quot; ## chr (9): session_id, plaform, segment, customer_type, device_group, visitor... ## dbl (12): ...1, has_listing, has_used_search, has_recommendation, has_add_to... ## ## iÂ Use `spec()` to retrieve the full column specification for this data. ## iÂ Specify the column types or set `show_col_types = FALSE` to quiet this message. balanced_data &lt;- read_csv(&quot;./data/train_balanced.csv&quot;) %&gt;% select(-...1, -session_id) %&gt;% mutate(bought = as.factor(bought)) ## New names: ## * `` -&gt; ...1 ## Rows: 16000 Columns: 21 ## -- Column specification -------------------------------------------------------- ## Delimiter: &quot;,&quot; ## chr (9): session_id, plaform, segment, customer_type, device_group, visitor... ## dbl (12): ...1, has_listing, has_used_search, has_recommendation, has_add_to... ## ## iÂ Use `spec()` to retrieve the full column specification for this data. ## iÂ Specify the column types or set `show_col_types = FALSE` to quiet this message. The findings from the data exploration stage will be used to preprocess the data to be used for each model. Each model requires a set of specific transformation and each algorithm treats inputs differently (eg: Nearest Neigbors are sensible to scale, while naives bayes is not) so the pre process should be consider on a model experiment base. Post-processing and model definition will be done on the next stage. A number of key prepocess transformations are considered: - feature selection and reducion. This is made using domain knowledge and looking at correlation between features. Nonetheless there is a test using full model. If not based on domain knowledge will use PCA transform. imputation of missing data when needed Scale and center features when required by the model Transform skwed features specially for models where variance plays a role or rely on gaussian (linear model). Given the business focus of interpreting the model (focus on inference) when possible will use log transform instead of more complex Box Plot transforms \\(\\lambda\\). 3.1 Summary of transformation steps identified during EDA The following transformations were identified during the EDA stage: Features Device Group and Platform showed a strong relationship between them. Given that Platform provides more information Device Group will be removed Features starting with has_ refers to milestones on customer journey, therefore the need to include all of them should be taken in consideration. We will use PCA to merge this into one feature and experiment between the use of them individual or together, Duration feature is Right Skewed and contains observations which lack business sense. To avoid any impact on the model, specially on ones more sensible to variance, it will be log transformed and observations above 24hours will be removed Replace all Missing values on country, browser_name and is_subscribed using Nearest Neighbors imputation with a very a k of 3, Reduce the number of observations of class country and browser_name by compressing smaller ones into a generic class named others. This will be done because some models do not manage very well the fact that some dummy variables only have one level, Reduce the number of levels of Segment by re factoring into a class of has or not segment, Specially for the use of Neural Networks and Supervisor Vector Machines, numeric variables will be normalized and scaled Converts unique_browse_designer_qty, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty to ratios over Total Pageviews (view_qty) which provides information on the amount of page views actually related to that category and subsequently log transform, Sessions with duration lower than 2 seconds and a total page view of 0 or 1 will be removed # For balanced data and to be used generically for all models preprocess &lt;- recipe(bought ~ . , data = balanced_data) %&gt;% step_ratio(starts_with(&quot;unique_&quot;), denom = denom_vars(view_qty)) %&gt;% step_rm(device_group, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty) %&gt;% step_other(country, browser_name, threshold = 0.05) %&gt;% step_log(view_qty) %&gt;% step_filter( duration &lt; 60 * 60 * 24, !((view_qty &lt;= 1) &amp; (duration &lt;= 2)) ) %&gt;% step_mutate(is_segment = ifelse(segment == &quot;without_segment&quot;, 0, 1)) %&gt;% step_rm(segment) %&gt;% step_string2factor(all_nominal_predictors()) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% step_pca(starts_with(&quot;has_&quot;), treshold = 0.8) %&gt;% step_corr(all_numeric_predictors(), threshold = .5) preprocess_logit_nn &lt;- preprocess %&gt;% step_scale(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) preprocess_mlp_svm &lt;- preprocess %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_scale(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) tidy(preprocess) ## # A tibble: 11 x 6 ## number operation type trained skip id ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 1 step ratio FALSE FALSE ratio_KuUj3 ## 2 2 step rm FALSE FALSE rm_HIp6r ## 3 3 step other FALSE FALSE other_9RvcY ## 4 4 step log FALSE FALSE log_tfGrx ## 5 5 step filter FALSE TRUE filter_7bO7w ## 6 6 step mutate FALSE FALSE mutate_8qFw1 ## 7 7 step rm FALSE FALSE rm_M0oH1 ## 8 8 step string2factor FALSE FALSE string2factor_UKyRw ## 9 9 step impute_knn FALSE FALSE impute_knn_44PTL ## 10 10 step pca FALSE FALSE pca_Nt9Z9 ## 11 11 step corr FALSE FALSE corr_uigwe tidy(preprocess_logit_nn) ## # A tibble: 13 x 6 ## number operation type trained skip id ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 1 step ratio FALSE FALSE ratio_KuUj3 ## 2 2 step rm FALSE FALSE rm_HIp6r ## 3 3 step other FALSE FALSE other_9RvcY ## 4 4 step log FALSE FALSE log_tfGrx ## 5 5 step filter FALSE TRUE filter_7bO7w ## 6 6 step mutate FALSE FALSE mutate_8qFw1 ## 7 7 step rm FALSE FALSE rm_M0oH1 ## 8 8 step string2factor FALSE FALSE string2factor_UKyRw ## 9 9 step impute_knn FALSE FALSE impute_knn_44PTL ## 10 10 step pca FALSE FALSE pca_Nt9Z9 ## 11 11 step corr FALSE FALSE corr_uigwe ## 12 12 step scale FALSE FALSE scale_27eL5 ## 13 13 step dummy FALSE FALSE dummy_Daujr tidy(preprocess_mlp_svm) ## # A tibble: 15 x 6 ## number operation type trained skip id ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 1 step ratio FALSE FALSE ratio_KuUj3 ## 2 2 step rm FALSE FALSE rm_HIp6r ## 3 3 step other FALSE FALSE other_9RvcY ## 4 4 step log FALSE FALSE log_tfGrx ## 5 5 step filter FALSE TRUE filter_7bO7w ## 6 6 step mutate FALSE FALSE mutate_8qFw1 ## 7 7 step rm FALSE FALSE rm_M0oH1 ## 8 8 step string2factor FALSE FALSE string2factor_UKyRw ## 9 9 step impute_knn FALSE FALSE impute_knn_44PTL ## 10 10 step pca FALSE FALSE pca_Nt9Z9 ## 11 11 step corr FALSE FALSE corr_uigwe ## 12 12 step normalize FALSE FALSE normalize_GGAvn ## 13 13 step scale FALSE FALSE scale_rWpMt ## 14 14 step dummy FALSE FALSE dummy_tPOdH ## 15 15 step zv FALSE FALSE zv_fXtpm preprocess_data &lt;- recipe(bought ~ . , data = balanced_data) %&gt;% step_ratio(starts_with(&quot;unique_&quot;), denom = denom_vars(view_qty)) %&gt;% step_rm(device_group, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty) %&gt;% step_other(country, browser_name, threshold = 0.05) %&gt;% step_mutate(is_bounce = ifelse(view_qty == 1, 1, 0)) %&gt;% step_log(view_qty) %&gt;% step_filter(duration &lt; 60 * 60 * 24, !((view_qty &lt;= 1) &amp; (duration &lt;= 2))) %&gt;% step_mutate(is_segment = ifelse(segment == &quot;without_segment&quot;, 0, 1)) %&gt;% step_rm(segment) %&gt;% step_string2factor(all_nominal_predictors()) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% step_pca(starts_with(&quot;has_&quot;), num_comp = 1) %&gt;% step_corr(all_numeric_predictors(), threshold = .5) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_scale(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) %&gt;% prep() %&gt;% bake(new_data = NULL) summary(preprocess_data) ## duration bought unique_product_qty_o_view_qty is_bounce ## Min. :-0.7436 0:4087 Min. :-0.9715 Min. :-0.03526 ## 1st Qu.:-0.6329 1:7991 1st Qu.:-0.6674 1st Qu.:-0.03526 ## Median :-0.3610 Median :-0.2637 Median :-0.03526 ## Mean : 0.0000 Mean : 0.0000 Mean : 0.00000 ## 3rd Qu.: 0.2597 3rd Qu.: 0.3341 3rd Qu.:-0.03526 ## Max. :11.7405 Max. : 3.8947 Max. :28.35725 ## is_segment PC1 plaform_website customer_type_prospect ## Min. :-0.9183 Min. :-1.5186 Min. :0.0000 Min. :0.0000 ## 1st Qu.:-0.9183 1st Qu.:-0.7680 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :-0.9183 Median :-0.0134 Median :0.0000 Median :0.0000 ## Mean : 0.0000 Mean : 0.0000 Mean :0.4945 Mean :0.4822 ## 3rd Qu.: 1.0888 3rd Qu.: 0.7120 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. : 1.0888 Max. : 1.6572 Max. :1.0000 Max. :1.0000 ## visitor_type_returning is_subscribed_Unknown is_subscribed_Yes ## Min. :0.0000 Min. :0.00000 Min. :0.0000 ## 1st Qu.:1.0000 1st Qu.:0.00000 1st Qu.:0.0000 ## Median :1.0000 Median :0.00000 Median :1.0000 ## Mean :0.7606 Mean :0.00149 Mean :0.6239 ## 3rd Qu.:1.0000 3rd Qu.:0.00000 3rd Qu.:1.0000 ## Max. :1.0000 Max. :1.00000 Max. :1.0000 ## browser_name_Safari browser_name_other country_RU country_US ## Min. :0.0000 Min. :0.0000 Min. :0.0000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :0.0000 Median :0.0000 Median :0.0000 Median :0.0000 ## Mean :0.4369 Mean :0.2013 Mean :0.1177 Mean :0.2166 ## 3rd Qu.:1.0000 3rd Qu.:0.0000 3rd Qu.:0.0000 3rd Qu.:0.0000 ## Max. :1.0000 Max. :1.0000 Max. :1.0000 Max. :1.0000 ## country_other ## Min. :0.0000 ## 1st Qu.:0.0000 ## Median :1.0000 ## Mean :0.6018 ## 3rd Qu.:1.0000 ## Max. :1.0000 "],["modeling.html", "Stage 4 Modeling 4.1 Baseline 4.2 Naive Bayes Classifier 4.3 Logistic Regression 4.4 Knn 4.5 Workflow: Rpart 4.6 Workflow: Random Forest 4.7 Workflow: Single Layer Neural Network 4.8 Support Vector Machine", " Stage 4 Modeling # libraries library(tidyverse) library(tidymodels) library(keras) ## ## Attaching package: &#39;keras&#39; ## The following object is masked from &#39;package:yardstick&#39;: ## ## get_weights library(workflowsets) library(discrim) ## ## Attaching package: &#39;discrim&#39; ## The following object is masked from &#39;package:dials&#39;: ## ## smoothness library(factoextra) ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa # cv 10 folds set.seed(1234) # To keep speed of processing no repeats will be used during resampling # folds &lt;- vfold_cv(balanced_data, v = 10, repeats = 5) folds &lt;- vfold_cv(balanced_data, v = 10) The following planning will be used for a list of candidate models: A Model baseline will be defined and will be used to compare against other models Define training and evaluation series. Using Cross Validation with 10 folds Initial experiment using preprocessing workflow When needed given this project objectives hyperparameters will be tuned Retrain model with new hyperparameters Model comparison Given the goals set for this project we shall focus on Accuracy and Auc ROC as metrics to compare models. The initial experiment uses the preprocess schema and the default hyperparameters for each model. Given the name of the files available and since no more information is available, the subsquent teste will be done assuming that a test split was already made and the available information is only for trainning. Therefore, the cross validation resampling will be done over this training dataset. 4.1 Baseline Given its simplicity and flexibility a Naive Bayes classifier will be used as baseline for this project. The preprocess was set to a minimum in order to measure the impact it might have on the final output. The engine used for this algorithm does not accept missing data, therefore we did a 3nn imputation. # 1. specify the model naive_Bayes &lt;- discrim::naive_Bayes() %&gt;% set_engine(&#39;klaR&#39;) # 2. preprocessing preprocess &lt;- recipe(bought ~ ., data = balanced_data) %&gt;% step_string2factor(all_nominal_predictors()) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% step_scale(all_numeric_predictors()) # 3, Buildworkflow baseline_wflow &lt;- workflow() %&gt;% add_model(naive_Bayes) %&gt;% add_recipe(preprocess) #4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) baseline_fit &lt;- baseline_wflow %&gt;% fit_resamples(folds, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; ## ! Fold01: preprocessor 1/1, model 1/1 (predictions): Numerical 0 probability for a... ## ! Fold02: preprocessor 1/1, model 1/1 (predictions): Numerical 0 probability for a... ## ! Fold03: preprocessor 1/1, model 1/1 (predictions): Numerical 0 probability for a... ## ! Fold04: preprocessor 1/1, model 1/1 (predictions): Numerical 0 probability for a... ## ! Fold05: preprocessor 1/1, model 1/1 (predictions): Numerical 0 probability for a... ## ! Fold06: preprocessor 1/1, model 1/1 (predictions): Numerical 0 probability for a... ## ! Fold07: preprocessor 1/1, model 1/1 (predictions): Numerical 0 probability for a... ## ! Fold08: preprocessor 1/1, model 1/1 (predictions): Numerical 0 probability for a... ## ! Fold09: preprocessor 1/1, model 1/1 (predictions): Numerical 0 probability for a... ## ! Fold10: preprocessor 1/1, model 1/1 (predictions): Numerical 0 probability for a... #5. Performance metrics over the validation set collect_metrics(baseline_fit) ## # A tibble: 2 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.816 10 0.00246 Preprocessor1_Model1 ## 2 roc_auc binary 0.914 10 0.00150 Preprocessor1_Model1 conf_mat_resampled(baseline_fit) ## # A tibble: 4 x 3 ## Prediction Truth Freq ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0 689. ## 2 0 1 183. ## 3 1 0 111. ## 4 1 1 617. 4.2 Naive Bayes Classifier # libraries library(tidyverse) library(tidymodels) library(keras) library(workflowsets) library(discrim) library(factoextra) # cv 10 folds set.seed(1234) # To keep speed of processing no repeats will be used during resampling # folds &lt;- vfold_cv(balanced_data, v = 10, repeats = 5) folds &lt;- vfold_cv(balanced_data, v = 10) 4.2.1 Initial experiments Following the baseline but introducing pre-processing # 1. specify the model naive_Bayes &lt;- discrim::naive_Bayes() %&gt;% set_engine(&#39;klaR&#39;) # 2. preprocessing preprocess &lt;- recipe(bought ~ . , data = balanced_data) %&gt;% step_ratio(starts_with(&quot;unique_&quot;), denom = denom_vars(view_qty)) %&gt;% step_rm(device_group, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty) %&gt;% step_other(country, browser_name, threshold = 0.05) %&gt;% step_log(view_qty) %&gt;% step_filter( duration &lt; 60 * 60 * 24, !((view_qty &lt;= 1) &amp; (duration &lt;= 2)) ) %&gt;% step_string2factor(all_nominal_predictors()) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% step_pca(starts_with(&quot;has_&quot;), num_comp = 1) %&gt;% step_corr(all_numeric_predictors(), threshold = .5) preprocess_logit_nn &lt;- preprocess %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_scale(all_numeric_predictors()) # 3, Buildworkflow nbayes_wflow &lt;- workflow() %&gt;% add_model(naive_Bayes) %&gt;% add_recipe(preprocess_logit_nn) #4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) nbayes_fit &lt;- nbayes_wflow %&gt;% fit_resamples(folds, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; #5. Performance metrics over the validation set collect_metrics(nbayes_fit) ## # A tibble: 2 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.872 10 0.00428 Preprocessor1_Model1 ## 2 roc_auc binary 0.943 10 0.00195 Preprocessor1_Model1 # export files save(nbayes_fit, file = &quot;_models/nbayes_fit.RData&quot;) 4.3 Logistic Regression # libraries library(tidyverse) library(tidymodels) library(keras) library(workflowsets) library(discrim) library(factoextra) # cv 10 folds set.seed(1234) # To keep speed of processing no repeats will be used during resampling # folds &lt;- vfold_cv(balanced_data, v = 10, repeats = 5) folds &lt;- vfold_cv(balanced_data, v = 10) 4.3.1 Initial experiments Using library glml # 1. specify the model logistic_reg_glm_spec &lt;- logistic_reg() %&gt;% set_engine(&#39;glm&#39;) # 2. preprocessing preprocess &lt;- recipe(bought ~ . , data = balanced_data) %&gt;% step_ratio(starts_with(&quot;unique_&quot;), denom = denom_vars(view_qty)) %&gt;% step_rm(device_group, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty) %&gt;% step_other(country, browser_name, threshold = 0.05) %&gt;% step_log(view_qty) %&gt;% step_filter( duration &lt; 60 * 60 * 24, !((view_qty &lt;= 1) &amp; (duration &lt;= 2)) ) %&gt;% step_string2factor(all_nominal_predictors()) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% step_pca(starts_with(&quot;has_&quot;), num_comp = 1) %&gt;% step_corr(all_numeric_predictors(), threshold = .5) preprocess_logit_nn &lt;- preprocess %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_scale(all_numeric_predictors()) # 3, Buildworkflow logit_wflow &lt;- workflow() %&gt;% add_model(logistic_reg_glm_spec) %&gt;% add_recipe(preprocess_logit_nn) #4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) logit_fit &lt;- logit_wflow %&gt;% fit_resamples(folds, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; #5. Performance metrics over the validation set collect_metrics(logit_fit) ## # A tibble: 2 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.823 10 0.00279 Preprocessor1_Model1 ## 2 roc_auc binary 0.920 10 0.00202 Preprocessor1_Model1 # export files save(logit_fit, file = &quot;_models/logit_fit.RData&quot;) 4.4 Knn # libraries library(tidyverse) library(tidymodels) library(workflowsets) library(discrim) library(factoextra) # cv 10 folds set.seed(1234) # To keep speed of processing no repeats will be used during resampling # folds &lt;- vfold_cv(balanced_data, v = 10, repeats = 5) folds &lt;- vfold_cv(balanced_data, v = 10) 4.4.1 Initial experiment # 1. specify the model kknn &lt;- nearest_neighbor(neighbors = 3) %&gt;% set_engine(&#39;kknn&#39;) %&gt;% set_mode(&#39;classification&#39;) # 2. preprocessing preprocess &lt;- recipe(bought ~ . , data = balanced_data) %&gt;% step_ratio(starts_with(&quot;unique_&quot;), denom = denom_vars(view_qty)) %&gt;% step_rm(device_group, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty) %&gt;% step_other(country, browser_name, threshold = 0.05) %&gt;% step_log(view_qty) %&gt;% step_filter( duration &lt; 60 * 60 * 24, !((view_qty &lt;= 1) &amp; (duration &lt;= 2)) ) %&gt;% step_string2factor(all_nominal_predictors()) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% step_pca(starts_with(&quot;has_&quot;), num_comp = 1) %&gt;% step_corr(all_numeric_predictors(), threshold = .5) preprocess_logit_nn &lt;- preprocess %&gt;% step_scale(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) # 3, Buildworkflow kknn_wflow &lt;- workflow() %&gt;% add_model(kknn) %&gt;% add_recipe(preprocess_logit_nn) #4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) kknn_fit &lt;- kknn_wflow %&gt;% fit_resamples(folds, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; #5. Performance metrics over the validation set collect_metrics(kknn_fit) ## # A tibble: 2 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.871 10 0.00269 Preprocessor1_Model1 ## 2 roc_auc binary 0.922 10 0.00220 Preprocessor1_Model1 kknn_fit %&gt;% collect_predictions() %&gt;% group_by(id) %&gt;% roc_curve(bought, .pred_0) %&gt;% ggplot(aes(1- specificity, sensitivity, color = id)) + geom_abline(lty = 3, color = &quot;gray80&quot;, size = 1) + geom_path(show.legend = FALSE, alpha = 0.6, size = 1) + theme_masterDS() + coord_equal() 4.4.2 Fitting multiple models using grid search #1. list model to inject into workflow kknn_grid&lt;- nearest_neighbor(neighbors = tune()) %&gt;% set_engine(&#39;kknn&#39;) %&gt;% set_mode(&#39;classification&#39;) models_flow &lt;- workflow_set( preproc = list(recipe = preprocess_logit_nn), models = list(kknn_grid), cross = TRUE ) kknn_wf_fit &lt;- models_flow %&gt;% workflow_map( fn =&quot;tune_grid&quot;, resamples = folds, grid = 10 , metrics = metric_set(yardstick::roc_auc, yardstick::accuracy, yardstick::sens), verbose = TRUE ) ## i 1 of 1 tuning: recipe_nearest_neighbor ## v 1 of 1 tuning: recipe_nearest_neighbor (4m 21.7s) autoplot(kknn_wf_fit) + theme_masterDS() # export files save(kknn_fit, file = &quot;_models/kknn_fit.RData&quot;) save(kknn_wf_fit, file = &quot;_models/kknn_wf_grid.RData&quot;) 4.5 Workflow: Rpart # libraries library(tidyverse) library(tidymodels) library(keras) library(workflowsets) library(discrim) library(factoextra) # cv 10 folds set.seed(1234) # To keep speed of processing no repeats will be used during resampling # folds &lt;- vfold_cv(balanced_data, v = 10, repeats = 5) folds &lt;- vfold_cv(balanced_data, v = 10) 4.5.1 Initial experiment Most packages for tree-based models use the formula interface but do not encode the categorical predictors as dummy variables. # 1. specify the model tree_model &lt;- decision_tree(min_n = 2) %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) # 2. preprocessing preprocess &lt;- recipe(bought ~ . , data = balanced_data) %&gt;% step_ratio(starts_with(&quot;unique_&quot;), denom = denom_vars(view_qty)) %&gt;% step_rm(device_group, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty) %&gt;% step_other(country, browser_name, threshold = 0.05) %&gt;% step_log(view_qty) %&gt;% step_filter( duration &lt; 60 * 60 * 24, !((view_qty &lt;= 1) &amp; (duration &lt;= 2)) ) %&gt;% step_string2factor(all_nominal_predictors()) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% step_pca(starts_with(&quot;has_&quot;), num_comp = 1) %&gt;% step_corr(all_numeric_predictors(), threshold = .5) # 3, Buildworkflow tree_wflow &lt;- workflow() %&gt;% add_model(tree_model) %&gt;% add_recipe(preprocess) #4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) tree_fit &lt;- tree_wflow %&gt;% fit_resamples(folds, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; #5. Performance metrics over the validation set collect_metrics(tree_fit) ## # A tibble: 2 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.896 10 0.00195 Preprocessor1_Model1 ## 2 roc_auc binary 0.912 10 0.00145 Preprocessor1_Model1 tree_fit %&gt;% collect_predictions() %&gt;% group_by(id) %&gt;% roc_curve(bought, .pred_0) %&gt;% ggplot(aes(1- specificity, sensitivity, color = id)) + geom_abline(lty = 3, color = &quot;gray80&quot;, size = 1) + geom_path(show.legend = FALSE, alpha = 0.6, size = 1) + theme_masterDS() + coord_equal() # export files save(tree_fit, file = &quot;_models/tree_fit.RData&quot;) 4.6 Workflow: Random Forest # libraries library(tidyverse) library(tidymodels) library(keras) library(workflowsets) library(discrim) library(factoextra) # cv 10 folds set.seed(1234) # To keep speed of processing no repeats will be used during resampling # folds &lt;- vfold_cv(balanced_data, v = 10, repeats = 5) folds &lt;- vfold_cv(balanced_data, v = 10) 4.6.1 Initial experiment # 1. specify the model rand_forest &lt;- rand_forest(mtry = 1000) %&gt;% set_engine(&#39;ranger&#39;) %&gt;% set_mode(&#39;classification&#39;) # 2. preprocessing preprocess &lt;- recipe(bought ~ . , data = balanced_data) %&gt;% step_ratio(starts_with(&quot;unique_&quot;), denom = denom_vars(view_qty)) %&gt;% step_rm(device_group, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty) %&gt;% step_other(country, browser_name, threshold = 0.05) %&gt;% step_log(view_qty) %&gt;% step_filter( duration &lt; 60 * 60 * 24, !((view_qty &lt;= 1) &amp; (duration &lt;= 2)) ) %&gt;% step_string2factor(all_nominal_predictors()) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% step_pca(starts_with(&quot;has_&quot;), num_comp = 1) %&gt;% step_corr(all_numeric_predictors(), threshold = .5) # 3, Buildworkflow forest_wflow &lt;- workflow() %&gt;% add_model(rand_forest) %&gt;% add_recipe(preprocess) #4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) forest_fit &lt;- forest_wflow %&gt;% fit_resamples(folds, verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; ## ! Fold01: preprocessor 1/1, model 1/1: 1000 columns were requested but there were ... ## ! Fold02: preprocessor 1/1, model 1/1: 1000 columns were requested but there were ... ## ! Fold03: preprocessor 1/1, model 1/1: 1000 columns were requested but there were ... ## ! Fold04: preprocessor 1/1, model 1/1: 1000 columns were requested but there were ... ## ! Fold05: preprocessor 1/1, model 1/1: 1000 columns were requested but there were ... ## ! Fold06: preprocessor 1/1, model 1/1: 1000 columns were requested but there were ... ## ! Fold07: preprocessor 1/1, model 1/1: 1000 columns were requested but there were ... ## ! Fold08: preprocessor 1/1, model 1/1: 1000 columns were requested but there were ... ## ! Fold09: preprocessor 1/1, model 1/1: 1000 columns were requested but there were ... ## ! Fold10: preprocessor 1/1, model 1/1: 1000 columns were requested but there were ... #5. Performance metrics over the validation set collect_metrics(forest_fit) ## # A tibble: 2 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.924 10 0.00278 Preprocessor1_Model1 ## 2 roc_auc binary 0.972 10 0.00108 Preprocessor1_Model1 forest_fit %&gt;% collect_predictions() %&gt;% group_by(id) %&gt;% roc_curve(bought, .pred_0) %&gt;% ggplot(aes(1- specificity, sensitivity, color = id)) + geom_abline(lty = 3, color = &quot;gray80&quot;, size = 1) + geom_path(show.legend = FALSE, alpha = 0.6, size = 1) + theme_masterDS() + coord_equal() 4.6.2 Fitting multiple models using grid search #1. list model to inject into workflow rand_forest_grid &lt;- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %&gt;% set_engine(&#39;ranger&#39;) %&gt;% set_mode(&#39;classification&#39;) models_flow &lt;- workflow_set( preproc = list(recipe = preprocess), models = list(rand_forest_grid), cross = TRUE ) forest_wf_fit &lt;- models_flow %&gt;% workflow_map( fn =&quot;tune_grid&quot;, resamples = folds, grid = 10 , metrics = metric_set(yardstick::roc_auc, yardstick::accuracy, yardstick::sens), verbose = TRUE ) ## i 1 of 1 tuning: recipe_rand_forest ## i Creating pre-processing data to finalize unknown parameter: mtry ## v 1 of 1 tuning: recipe_rand_forest (26m 25.1s) autoplot(forest_wf_fit) + theme_masterDS() # export files save(forest_fit, file = &quot;_models/forest_fit.RData&quot;) save(forest_wf_fit, file = &quot;_models/forest_wf_grif.RData&quot;) 4.7 Workflow: Single Layer Neural Network # libraries library(tidyverse) library(tidymodels) library(workflowsets) library(factoextra) # cv 10 folds set.seed(1234) # To keep speed of processing no repeats will be used during resampling # folds &lt;- vfold_cv(balanced_data, v = 10, repeats = 5) folds &lt;- vfold_cv(balanced_data, v = 10) 4.7.1 Initial experiment # 1. specify the model sl_nnet &lt;- mlp(hidden_units = NULL, penalty = NULL, epochs = NULL) %&gt;% set_engine(&#39;nnet&#39;) %&gt;% set_mode(&#39;classification&#39;) # 2. preprocessing preprocess &lt;- recipe(bought ~ . , data = balanced_data) %&gt;% step_ratio(starts_with(&quot;unique_&quot;), denom = denom_vars(view_qty)) %&gt;% step_rm(device_group, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty) %&gt;% step_other(country, browser_name, threshold = 0.05) %&gt;% step_log(view_qty) %&gt;% step_filter( duration &lt; 60 * 60 * 24, !((view_qty &lt;= 1) &amp; (duration &lt;= 2)) ) %&gt;% step_string2factor(all_nominal_predictors()) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% step_pca(starts_with(&quot;has_&quot;), num_comp = 1) %&gt;% step_corr(all_numeric_predictors(), threshold = .5) preprocess_mlp_svm &lt;- preprocess %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_scale(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) # 3, Buildworkflow sl_wflow &lt;- workflow() %&gt;% add_model(sl_nnet) %&gt;% add_recipe(preprocess_mlp_svm) #4. Fit model fit_control &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE) sl_fit &lt;- sl_wflow %&gt;% fit_resamples( resamples = folds, metrics = metric_set(yardstick::roc_auc, yardstick::accuracy, yardstick::sens, yardstick::specificity), verbose = TRUE, control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; #5. Performance metrics over the validation set collect_metrics(sl_fit) ## # A tibble: 4 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.911 10 0.00329 Preprocessor1_Model1 ## 2 roc_auc binary 0.963 10 0.00184 Preprocessor1_Model1 ## 3 sens binary 0.872 10 0.00596 Preprocessor1_Model1 ## 4 specificity binary 0.950 10 0.00319 Preprocessor1_Model1 sl_fit %&gt;% collect_predictions() %&gt;% group_by(id) %&gt;% roc_curve(bought, .pred_0) %&gt;% ggplot(aes(1- specificity, sensitivity, color = id)) + geom_abline(lty = 3, color = &quot;gray80&quot;, size = 1) + geom_path(show.legend = FALSE, alpha = 0.6, size = 1) + theme_masterDS() + coord_equal() 4.7.2 Fitting multiple models using grid search #1. list model to inject into workflow sl_nnet &lt;- mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;% set_engine(&#39;nnet&#39;) %&gt;% set_mode(&#39;classification&#39;) models_flow &lt;- workflow_set( preproc = list(recipe = preprocess_mlp_svm), models = list(sl_nnet), cross = TRUE ) nn_wf_fit &lt;- models_flow %&gt;% workflow_map( fn =&quot;tune_grid&quot;, resamples = folds, grid = 10 , metrics = metric_set(yardstick::roc_auc, yardstick::sens, yardstick::accuracy,yardstick::spec), verbose = TRUE ) ## i 1 of 1 tuning: recipe_mlp ## v 1 of 1 tuning: recipe_mlp (14m 45.8s) autoplot(nn_wf_fit) + theme_masterDS() autoplot(nn_wf_fit, select_best = TRUE) + theme_masterDS() rank_results(nn_wf_fit, rank_metric = &quot;sens&quot;, select_best = TRUE) ## # A tibble: 4 x 9 ## wflow_id .config .metric mean std_err n preprocessor model rank ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 recipe_mlp Preprocessor1~ accura~ 0.913 0.00334 10 recipe mlp 1 ## 2 recipe_mlp Preprocessor1~ roc_auc 0.967 0.00204 10 recipe mlp 1 ## 3 recipe_mlp Preprocessor1~ sens 0.888 0.00452 10 recipe mlp 1 ## 4 recipe_mlp Preprocessor1~ spec 0.939 0.00368 10 recipe mlp 1 # export files save(sl_fit, file = &quot;_models/sl_fit.RData&quot;) save(nn_wf_fit, file = &quot;_models/nn_wf_fit.RData&quot;) 4.8 Support Vector Machine 4.8.1 Initial experience: Linear Kernel A Linear Kernel with default parameters will be used as the basis for # 1. specify the model svm_linear &lt;- svm_linear(cost = 0.5) %&gt;% set_engine(&quot;kernlab&quot;) %&gt;% set_mode(&quot;classification&quot;) # 2. preprocessing preprocess &lt;- recipe(bought ~ . , data = balanced_data) %&gt;% step_ratio(starts_with(&quot;unique_&quot;), denom = denom_vars(view_qty)) %&gt;% step_rm(device_group, unique_product_qty, unique_browse_designer_qty, unique_browse_category_qty) %&gt;% step_other(country, browser_name, threshold = 0.05) %&gt;% step_log(view_qty) %&gt;% step_filter( duration &lt; 60 * 60 * 24, !((view_qty &lt;= 1) &amp; (duration &lt;= 2)) ) %&gt;% step_string2factor(all_nominal_predictors()) %&gt;% step_impute_knn(all_predictors(), neighbors = 3) %&gt;% step_pca(starts_with(&quot;has_&quot;), num_comp = 1) %&gt;% step_corr(all_numeric_predictors(), threshold = .5) preprocess_mlp_svm &lt;- preprocess %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_scale(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_zv(all_predictors()) # 3, Buildworkflow svm_linear_wflow &lt;- workflow() %&gt;% add_model(svm_linear) %&gt;% add_recipe(preprocess_mlp_svm) #4. Fit model fit_control &lt;- control_grid(save_pred = TRUE, save_workflow = TRUE) svm_linear_fit &lt;- svm_linear_wflow %&gt;% fit_resamples( resamples = folds, verbose = TRUE, metrics = metric_set(yardstick::roc_auc, yardstick::accuracy, yardstick::sens,yardstick::specificity), control = fit_control) ## Warning: The `...` are not used in this function but one or more objects were ## passed: &#39;verbose&#39; #5. Performance metrics over the validation set collect_metrics(svm_linear_fit) ## # A tibble: 4 x 6 ## .metric .estimator mean n std_err .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 accuracy binary 0.805 10 0.00403 Preprocessor1_Model1 ## 2 roc_auc binary 0.919 10 0.00202 Preprocessor1_Model1 ## 3 sens binary 0.673 10 0.00733 Preprocessor1_Model1 ## 4 specificity binary 0.937 10 0.00215 Preprocessor1_Model1 conf_mat_resampled(svm_linear_fit) ## # A tibble: 4 x 3 ## Prediction Truth Freq ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0 538. ## 2 0 1 50.2 ## 3 1 0 262. ## 4 1 1 750. svm_linear_fit %&gt;% collect_predictions() %&gt;% group_by(id) %&gt;% roc_curve(bought, .pred_0) %&gt;% ggplot(aes(1- specificity, sensitivity, color = id)) + geom_abline(lty = 3, color = &quot;gray80&quot;, size = 1) + geom_path(show.legend = FALSE, alpha = 0.6, size = 1) + theme_masterDS() + coord_equal() 4.8.2 Fitting multiple models #1. list model to inject into workflow svm_linear &lt;- svm_rbf(cost = tune()) %&gt;% set_engine(&quot;kernlab&quot;) %&gt;% set_mode(&quot;classification&quot;) svm_poly_kernlab_spec &lt;- svm_poly(cost = tune(), degree = tune(), scale_factor = tune(), margin = tune()) %&gt;% set_engine(&#39;kernlab&#39;) %&gt;% set_mode(&#39;classification&#39;) svm_rbf_kernlab_spec &lt;- svm_rbf(cost = tune(), rbf_sigma = tune(), margin = tune()) %&gt;% set_engine(&#39;kernlab&#39;) %&gt;% set_mode(&#39;classification&#39;) models_flow &lt;- workflow_set( preproc = list(recipe = preprocess_mlp_svm), models = list(svm_ploy = svm_poly_kernlab_spec, svm_rbf = svm_rbf_kernlab_spec, linear = svm_linear), cross = TRUE ) svm_wf_fit &lt;- models_flow %&gt;% workflow_map( fn =&quot;tune_grid&quot;, resamples = folds, grid = 5, metrics = metric_set(yardstick::roc_auc, yardstick::accuracy, yardstick::sens, yardstick::spec), verbose = TRUE ) ## i 1 of 3 tuning: recipe_svm_ploy ## v 1 of 3 tuning: recipe_svm_ploy (7m 34.9s) ## i 2 of 3 tuning: recipe_svm_rbf ## v 2 of 3 tuning: recipe_svm_rbf (14m 37.1s) ## i 3 of 3 tuning: recipe_linear ## v 3 of 3 tuning: recipe_linear (12m 17.3s) autoplot(svm_wf_fit) + theme_masterDS() autoplot(svm_wf_fit, select_best = TRUE) + theme_masterDS() rank_results(svm_wf_fit, rank_metric = &quot;sens&quot;, select_best = TRUE) ## # A tibble: 12 x 9 ## wflow_id .config .metric mean std_err n preprocessor model rank ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 recipe_linear Preproc~ accura~ 0.891 0.00294 9 recipe svm_~ 1 ## 2 recipe_linear Preproc~ roc_auc 0.952 0.00224 9 recipe svm_~ 1 ## 3 recipe_linear Preproc~ sens 0.840 0.00602 9 recipe svm_~ 1 ## 4 recipe_linear Preproc~ spec 0.943 0.00231 9 recipe svm_~ 1 ## 5 recipe_svm_ploy Preproc~ accura~ 0.879 0.00314 10 recipe svm_~ 2 ## 6 recipe_svm_ploy Preproc~ roc_auc 0.949 0.00228 10 recipe svm_~ 2 ## 7 recipe_svm_ploy Preproc~ sens 0.820 0.00548 10 recipe svm_~ 2 ## 8 recipe_svm_ploy Preproc~ spec 0.938 0.00284 10 recipe svm_~ 2 ## 9 recipe_svm_rbf Preproc~ accura~ 0.783 0.00446 10 recipe svm_~ 3 ## 10 recipe_svm_rbf Preproc~ roc_auc 0.919 0.00192 10 recipe svm_~ 3 ## 11 recipe_svm_rbf Preproc~ sens 0.595 0.00657 10 recipe svm_~ 3 ## 12 recipe_svm_rbf Preproc~ spec 0.972 0.00143 10 recipe svm_~ 3 rank_results(svm_wf_fit, rank_metric = &quot;accuracy&quot;, select_best = TRUE) ## # A tibble: 12 x 9 ## wflow_id .config .metric mean std_err n preprocessor model rank ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 recipe_linear Preproc~ accura~ 0.891 0.00294 9 recipe svm_~ 1 ## 2 recipe_linear Preproc~ roc_auc 0.952 0.00224 9 recipe svm_~ 1 ## 3 recipe_linear Preproc~ sens 0.840 0.00602 9 recipe svm_~ 1 ## 4 recipe_linear Preproc~ spec 0.943 0.00231 9 recipe svm_~ 1 ## 5 recipe_svm_ploy Preproc~ accura~ 0.879 0.00314 10 recipe svm_~ 2 ## 6 recipe_svm_ploy Preproc~ roc_auc 0.949 0.00228 10 recipe svm_~ 2 ## 7 recipe_svm_ploy Preproc~ sens 0.820 0.00548 10 recipe svm_~ 2 ## 8 recipe_svm_ploy Preproc~ spec 0.938 0.00284 10 recipe svm_~ 2 ## 9 recipe_svm_rbf Preproc~ accura~ 0.783 0.00446 10 recipe svm_~ 3 ## 10 recipe_svm_rbf Preproc~ roc_auc 0.919 0.00192 10 recipe svm_~ 3 ## 11 recipe_svm_rbf Preproc~ sens 0.595 0.00657 10 recipe svm_~ 3 ## 12 recipe_svm_rbf Preproc~ spec 0.972 0.00143 10 recipe svm_~ 3 autoplot(svm_wf_fit, metric = &quot;accuracy&quot;,id = &quot;recipe_svm_ploy&quot;) # export files save(svm_wf_fit,file=&quot;_models/svm_wf_fit.RData&quot;) save(svm_linear_fit, file = &quot;_models/svm_linear_fit.RData&quot;) "],["evaluation-and-conclusion.html", "Stage 5 Evaluation and conclusion 5.1 Next steps.", " Stage 5 Evaluation and conclusion From the initial experiments with different concurrent models the following results were achieved: # Combine all initial experiments experiments union_metrics &lt;- rbind( collect_metrics(nbayes_fit) %&gt;% mutate(model = &quot;Naive Bayes Classifier&quot;), collect_metrics(logit_fit) %&gt;% mutate(model = &quot;Logistic Regression&quot;), collect_metrics(kknn_fit) %&gt;% mutate(model = &quot;Nearest Neighbors&quot;), collect_metrics(tree_fit) %&gt;% mutate(model = &quot;Decision tree&quot;), collect_metrics(forest_fit) %&gt;% mutate(model = &quot;Random Forest&quot;), collect_metrics(sl_fit) %&gt;% mutate(model = &quot;Neural Network&quot;), collect_metrics(svm_linear_fit) %&gt;% mutate(model = &quot;Support Vector machine (linear kernel)&quot;) ) #baseline baseline &lt;- data.frame(.metric = c(&quot;accuracy&quot;, &quot;roc_auc&quot;),values = c(0.816, 0.913)) # find max data &lt;- filter( union_metrics, .metric %in% c(&quot;accuracy&quot;, &quot;roc_auc&quot;) ) %&gt;% group_by(.metric) %&gt;% mutate(color = mean == max(mean)) ggplot(data = data, aes(x = model, y = mean, fill = color)) + geom_bar(stat = &quot;identity&quot; ) + geom_hline(data = baseline, aes(yintercept = values), linetype = &quot;dashed&quot;) + facet_grid(.metric ~ .) + theme_masterDS() + labs( x = &quot;&quot;, y = &quot;&quot;, title = &quot;Comparing metrics between models&quot;, caption = &quot;dotted lines represent baseline&quot; ) + theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) + scale_fill_brewer(palette = &quot;Pastel1&quot;) During the business understanding the most accurate model was defined has been the main goal for this project. Therefore, the model with the highest accuracy metric (other metrics like sensibility and specificity were calculated during the modeling stage) was the one answering the company needs. Based on the results Random Forest shows the most promising values and future developments would be made using this model. It not only presents the highest metric for both accuracy and AUC ROC as it provides a more understandable model when compared to others. Is worth noticing that the increase from a baseline of a simple model without any substantial preprocessing is not substantial making the case towards the use a simpler model, with simpler feature importance attribution and less need for heavy preprocessing. 5.1 Next steps. Where to go from here If no time constrains existed or this project was done on a real environment i would do the following differently: Business understanding and the discovery phase in general is key to a successful project. The available information raise doubts about the data and the expected use for the model. Although some questions can be assumed its of paramount importance to gain a deeper knowledge about the question the company, client, project, etc wishes to answer. It impacts the list of models to experiment, the metrics to focus, etc. A model is as good as the action it enacts. This is particularly key on settings similar to this project were we analyse marketing data. Modeling for a marketing team is not the same as modeling to trigger an automation pipeline. The first tend to prefer a inferring models which provide more information about the role of different features. In this context, black box models like Neural Networks might not be advisable. On a similar note, when testing linear models, it makes sense to communicate and study coefficients even if its not the model with the highest metrics. More information how data was gathered is needed for a better exploration. Many questions were raise during the EDA stage which needed to be assumed. Data exploration needed to be expanded with more context information. This is not independent from the previous statement but some industry specific features might exist which could make sense to include. Except for a random grid search, no hyperparameter tunning was done on this project. Random grid search is a first step when no more information is available to know on which direction to keep tuning the model. Next steps would include a manual grid search which would narrow the field of test for each model. On this project uses the default model engine for each model. Different machines can return different models and have different performances. Each model error (difference between real observation and prediction) would need to be studied to validate assumptions, quality of the model. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
