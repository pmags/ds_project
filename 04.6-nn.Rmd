---
title: "R Notebook"
output: html_notebook
---

## Workflow: Single Layer Neural Network


```{r }
# libraries
library(tidyverse)
library(tidymodels)
library(workflowsets)
library(factoextra)

```

```{r , message=TRUE, warning=FALSE, include=FALSE}
# import data and prepocess

# import and prepare data
# import data
unbalanced_data <- read_csv("./data/train_full.csv") %>% 
  dplyr::select(-...1, -session_id) %>% 
  dplyr::mutate(bought = as.factor(bought))
balanced_data <- read_csv("./data/train_balanced.csv") %>% 
  dplyr::select(-...1, -session_id) %>% 
  dplyr::mutate(bought = as.factor(bought))

# script with visual functions
source("scripts/eda_functions.r")


```

```{r , message=TRUE, warning=FALSE}

# cv 10 folds

set.seed(1234)
# To keep speed of processing no repeats will be used during resampling
# folds <- vfold_cv(balanced_data, v = 10, repeats = 5)

folds <- vfold_cv(balanced_data, v = 10)

```



### Initial experiment


```{r}
# 1. specify the model
sl_nnet <-
  mlp(hidden_units = NULL, penalty = NULL, epochs = NULL) %>%
  set_engine('nnet') %>%
  set_mode('classification') 


# 2. preprocessing 
preprocess <- 
 recipe(bought ~ . , data = balanced_data) %>%
  step_ratio(starts_with("unique_"), denom = denom_vars(view_qty)) %>% 
  step_rm(device_group, unique_product_qty, unique_browse_designer_qty, 
          unique_browse_category_qty) %>% 
  step_other(country, browser_name, threshold = 0.05) %>% 
  step_log(view_qty) %>%
  step_filter(
    duration < 60 * 60 * 24,
    !((view_qty <= 1) & (duration <= 2))
    ) %>%
  step_string2factor(all_nominal_predictors()) %>% 
  step_impute_knn(all_predictors(), neighbors = 3) %>%
  step_pca(starts_with("has_"), num_comp = 1) %>% 
  step_corr(all_numeric_predictors(), threshold = .5) 
  
  
preprocess_mlp_svm <- 
  preprocess %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors())

  
# 3, Buildworkflow
sl_wflow <- 
  workflow() %>% 
  add_model(sl_nnet) %>% 
  add_recipe(preprocess_mlp_svm)

#4. Fit model
fit_control <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

sl_fit <- 
  sl_wflow %>% 
  fit_resamples(
    resamples = folds, 
    metrics = metric_set(yardstick::roc_auc, yardstick::accuracy, yardstick::sens, yardstick::specificity),
    verbose = TRUE, 
    control = fit_control)


#5. Performance metrics over the validation set
collect_metrics(sl_fit)

```

```{r }

sl_fit %>% 
  collect_predictions() %>% 
  group_by(id) %>% 
  roc_curve(bought, .pred_0) %>% 
  ggplot(aes(1- specificity, sensitivity, color = id)) +
  geom_abline(lty = 3, color = "gray80", size = 1) +
  geom_path(show.legend = FALSE, alpha = 0.6, size = 1) +
  theme_masterDS() +
  coord_equal()

```

### Fitting multiple models using grid search

```{r }

#1. list model to inject into workflow
sl_nnet <-
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
  set_engine('nnet') %>%
  set_mode('classification')


models_flow <- 
  workflow_set(
    preproc = list(recipe = preprocess_mlp_svm),
    models = list(sl_nnet),
    cross = TRUE 
    )

nn_wf_fit <- 
  models_flow %>%
  workflow_map(
    fn ="tune_grid",
    resamples = folds,
    grid = 10 ,
    metrics = metric_set(yardstick::roc_auc, yardstick::sens, yardstick::accuracy,yardstick::spec),
    verbose = TRUE
    ) 

```

```{r}

autoplot(nn_wf_fit) +
  theme_masterDS()

```

```{r}
autoplot(nn_wf_fit, select_best = TRUE) +
  theme_masterDS()
```

```{r}
rank_results(nn_wf_fit, rank_metric = "sens", select_best = TRUE) 
```


```{r}
# export files

save(sl_fit, file = "_models/sl_fit.RData")
save(nn_wf_fit, file = "_models/nn_wf_fit.RData")

```